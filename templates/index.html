<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>EHI - Voice Assistant</title>
  <style>
    body {
      background: #0e0e0e;
      color: white;
      font-family: 'Segoe UI', sans-serif;
      text-align: center;
      padding-top: 10%;
    }
    #response {
      margin-top: 2rem;
      font-size: 1.5rem;
      max-width: 600px;
      margin-left: auto;
      margin-right: auto;
    }
    #mic {
      background: #444;
      color: white;
      border: none;
      padding: 1.2rem;
      border-radius: 50%;
      font-size: 2rem;
      cursor: pointer;
      margin-top: 2rem;
      box-shadow: 0 0 20px #0ff;
    }
    #mic:hover {
      background: #0ff;
      color: black;
    }
  </style>
</head>
<body>
  <h1>üé§ Talk to EHI</h1>
  <p>Click the mic and speak. EHI will answer with voice.</p>
  <button id="mic">üéôÔ∏è</button>
  <div id="response">EHI is ready...</div>

  <script>
    const mic = document.getElementById("mic");
    const responseBox = document.getElementById("response");

    const synth = window.speechSynthesis;
    const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
    recognition.lang = "en-US";
    recognition.interimResults = false;
    recognition.maxAlternatives = 1;

    let voiceReady = false;

    // Activate audio on first user interaction
    window.addEventListener("click", () => {
      if (!voiceReady) {
        const dummy = new SpeechSynthesisUtterance("Voice activated.");
        synth.speak(dummy);
        voiceReady = true;
      }
    }, { once: true });

    mic.onclick = () => {
      responseBox.textContent = "üéôÔ∏è Listening...";
      recognition.start();
    };

    recognition.onresult = async (event) => {
      const msg = event.results[0][0].transcript;
      responseBox.textContent = `üó£Ô∏è You: ${msg}`;

      try {
        const res = await fetch("/ask", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ message: msg })
        });

        const data = await res.json();
        responseBox.textContent = `ü§ñ EHI: ${data.response}`;
        if (voiceReady) playVoice(data.response);
      } catch (err) {
        console.error("Fetch failed:", err);
        responseBox.textContent = "‚ùå Error getting response.";
      }
    };

    recognition.onerror = () => {
      responseBox.textContent = "‚ùå Didn't catch that. Try again.";
    };

    async function playVoice(text) {
      const apiKey = "sk_0943c49310f66aefe0841401ddbaef91860f0cb671aaefb7";  // Keep this safe in prod!
      const voiceId = "EXAVITQu4vr4xnSDxMaL"; // Bella

      const response = await fetch(`https://api.elevenlabs.io/v1/text-to-speech/${voiceId}`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "xi-api-key": apiKey
        },
        body: JSON.stringify({
          text: text,
          model_id: "eleven_monolingual_v1",
          voice_settings: { stability: 0.7, similarity_boost: 0.8 }
        })
      });

      if (!response.ok) {
        const errText = await response.text();
        console.error("üî¥ ElevenLabs error:", errText);
        responseBox.textContent = "‚ùå Voice error: " + response.status;
        return;
      }

      const audioBlob = await response.blob();
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);
      audio.autoplay = true;
      audio.onplay = () => console.log("üîä EHI is speaking...");
      audio.onerror = (e) => {
        console.error("Audio playback failed:", e);
        responseBox.textContent = "‚ùå Audio playback error.";
      };
    }
  </script>
</body>
</html>
